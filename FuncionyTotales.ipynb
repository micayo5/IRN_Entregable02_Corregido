{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#INSTALACIÓN DE PAQUETES E IMPORTACION DE LIBRERÍAS\n",
        "from google.colab import drive\n",
        "import os\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from fiftyone import ViewField as F\n",
        "import fiftyone.core.utils as fou\n",
        "from collections import Counter\n",
        "from fiftyone.core.stages import F\n",
        "import fiftyone.brain as fob\n",
        "import shutil\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from dask import bag, diagnostics\n",
        "from urllib import request\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.utils import img_to_array, array_to_img, load_img\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import shutil\n",
        "\n",
        "class Conexion:\n",
        "    def __init__(self):\n",
        "        drive.mount('/content/gdrive')\n",
        "        self.dataset = fo.Dataset.from_images_dir(\"/content/gdrive/MyDrive/DATASET/mydataset\")\n",
        "        \n",
        "    def crearmodelo(self):\n",
        "        #CREACIÓN DEL MODELO \n",
        "        self.model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
        "\n",
        "        #PROCESO DE EMBEDDINGS\n",
        "        self.embeddings = self.dataset.compute_embeddings(self.model)\n",
        "\n",
        "        print(self.embeddings.shape)\n",
        "\n",
        "        #SIMILARYTY MATRIX\n",
        "        self.similarity_matrix = cosine_similarity(self.embeddings)\n",
        "\n",
        "        print(self.similarity_matrix.shape)\n",
        "        print(self.similarity_matrix)\n",
        "\n",
        "        n = len(self.similarity_matrix)\n",
        "\n",
        "        self.similarity_matrix = self.similarity_matrix - np.identity(n)\n",
        "\n",
        "        id_map = [s.id for s in self.dataset.select_fields([\"id\"])]\n",
        "\n",
        "        #CREACIÓN DEL SAMPLE MAX_SIMILARITY\n",
        "        for idx, self.sample in enumerate(self.dataset):\n",
        "            self.sample[\"max_similarity\"] = self.similarity_matrix[idx].max()\n",
        "            self.sample.save()\n",
        "\n",
        "    def elimimaduplicados(self):\n",
        "        self.dataset.match(F(\"max_similarity\")>0.98)\n",
        "        id_map = [s.id for s in self.dataset.select_fields([\"id\"])] \n",
        "\n",
        "        #ELIMINAR DUPLICADOS\n",
        "        thresh = 0.98\n",
        "        self.samples_to_remove = set()\n",
        "        self.samples_to_keep = set()\n",
        "\n",
        "        for idx, self.sample in enumerate(self.dataset):\n",
        "            if self.sample.id not in self.samples_to_remove:\n",
        "                # Keep the first instance of two duplicates\n",
        "                self.samples_to_keep.add(self.sample.id)\n",
        "                \n",
        "                self.dup_idxs = np.where(self.similarity_matrix[idx] > thresh)[0]\n",
        "                for self.dup in self.dup_idxs:\n",
        "                    # We kept the first instance so remove all other duplicates\n",
        "                    self.samples_to_remove.add(id_map[self.dup])\n",
        "\n",
        "                if len(self.dup_idxs) > 0:\n",
        "                    self.sample.tags.append(\"has_duplicates\")\n",
        "                    self.sample.save()\n",
        "            else:\n",
        "                self.sample.tags.append(\"duplicate\")\n",
        "                self.sample.save()\n",
        "\n",
        "        #EVALUACIÓN RÁPIDA DE IMÁGENES QUE COINCIDEN \n",
        "        #Iterate over the samples and compute their file hashes:\n",
        "\n",
        "        for self.sample in self.dataset:\n",
        "            self.sample[\"file_hash\"] = fou.compute_filehash(self.sample.filepath)\n",
        "            self.sample.save()\n",
        "\n",
        "        print(self.dataset)\n",
        "\n",
        "        filehash_counts = Counter(self.sample.file_hash for self.sample in self.dataset)\n",
        "        dup_filehashes = [k for k, v in filehash_counts.items() if v > 1]\n",
        "\n",
        "        print(\"Number of duplicate file hashes: %d\" % len(dup_filehashes))\n",
        "\n",
        "        dup_view = (self.dataset\n",
        "            # Extract samples with duplicate file hashes\n",
        "            .match(F(\"file_hash\").is_in(dup_filehashes))\n",
        "            # Sort by file hash so duplicates will be adjacent\n",
        "            .sort_by(\"file_hash\")\n",
        "        )\n",
        "\n",
        "        print(\"Number of images that have a duplicate: %d\" % len(dup_view))\n",
        "        print(\"Number of duplicates: %d\" % (len(dup_view) - len(dup_filehashes)))\n",
        "\n",
        "        print(\"Length of dataset before: %d\" % len(self.dataset))\n",
        "\n",
        "        _dup_filehashes = set()\n",
        "        for self.sample in dup_view:\n",
        "            if self.sample.file_hash not in _dup_filehashes:\n",
        "                _dup_filehashes.add(self.sample.file_hash)\n",
        "                continue\n",
        "\n",
        "            del self.dataset[self.sample.id]\n",
        "\n",
        "        print(\"Length of dataset after: %d\" % len(self.dataset))\n",
        "\n",
        "        # Verify that the dataset no longer contains any duplicates\n",
        "        print(\"Number of unique file hashes: %d\" % len({s.file_hash for s in self.dataset}))\n",
        "\n",
        "        self.dataset.delete_samples(list(self.samples_to_remove))\n",
        "        print(\"Luego de evaluar max_similarity: %d\" % len(self.dataset))\n",
        "\n",
        "        \n",
        "        # evaluar la uniqueness de la data\n",
        "        fob.compute_uniqueness(self.dataset)\n",
        "\n",
        "        # Sort in decreasing order of uniqueness (least unique at the end)\n",
        "        #de mayor a menor\n",
        "        similar_ARMA = self.dataset.sort_by(\"uniqueness\", reverse=True)\n",
        "\n",
        "        # Verify that the most unique sample has the maximal uniqueness of 1.0\n",
        "        #Se crea el array uniqueness_fine para almacenar los valores de uniqueness\n",
        "        uniqueness_fine = [self.sample.uniqueness for self.sample in self.dataset]\n",
        "        print(uniqueness_fine)\n",
        "        p=len(uniqueness_fine) \n",
        "        print (p)\n",
        "\n",
        "        #Se crea el array dup_uniqueness_fine para almacenar los valores de uniqueness > c\n",
        "        dup_uniqueness_fine = []\n",
        "        dup_uniqueness_bad = []\n",
        "        c = 0.2  #valor de referencia para ver cuanta unicidad minima es necesaria\n",
        "        for v in uniqueness_fine:\n",
        "          if v >= c:\n",
        "            dup_uniqueness_fine.append(v)\n",
        "            \n",
        "          if v < c:\n",
        "            dup_uniqueness_bad.append(v) #data uniqueness < 0.2\n",
        "\n",
        "        n=len(dup_uniqueness_fine)\n",
        "        m=len(dup_uniqueness_bad)\n",
        "        print(\"Número total de archivos al inicio:\" + str(p))\n",
        "        f=(n/p)*100\n",
        "        print(\"Number of files wich uniqueness is > 0.2: %d\" % n)\n",
        "        print(\"Percent of data that is fine > 0.2: %d\" % f )\n",
        "\n",
        "        #Se crea el array dup_uniqueness_fine_data para \n",
        "        #pasar de los valores de uniqueness a los nombres de las fotos\n",
        "        ten_best = [x.filepath for x in similar_ARMA.limit(n)]\n",
        "        dup_uniqueness_fine_data = []\n",
        "\n",
        "        for filepath in ten_best:\n",
        "            dup_uniqueness_fine_data.append(filepath.split('/')[-1])\n",
        "        print(dup_uniqueness_fine_data)\n",
        "\n",
        "        # Sort in decreasing order of uniqueness (least unique at the end)\n",
        "        #de menor a mayor\n",
        "        similar_ARMA = self.dataset.sort_by(\"uniqueness\", reverse=False)\n",
        "\n",
        "        #Se crea el array dup_uniqueness_bad_data para \n",
        "        #pasar de los valores de uniqueness a los nombres de las fotos\n",
        "        ten_best = [x.filepath for x in similar_ARMA.limit(m)]\n",
        "        dup_uniqueness_bad_data = []\n",
        "\n",
        "        for filepath in ten_best:\n",
        "            dup_uniqueness_bad_data.append(filepath.split('/')[-1])\n",
        "        print(dup_uniqueness_bad_data)\n",
        "\n",
        "        #Crear carpeta para copiar datos unicos\n",
        "        try:\n",
        "          # Directory\n",
        "          directory = 'UNIQUENESS_FINE_DATA'\n",
        "          print(directory)  \n",
        "          # Parent Directory path\n",
        "          parent_dir = \"/content/gdrive/MyDrive/DATASET\"\n",
        "          path = os.path.join(parent_dir, directory)\n",
        "          print(path)\n",
        "          os.mkdir(path)\n",
        "        except:\n",
        "          print(\"Error, la carpeta no se puede crear o ya existe\" )\n",
        "\n",
        "        #Crear carpeta para copiar datos NO unicos\n",
        "        try:\n",
        "          # Directory\n",
        "          directory = 'UNIQUENESS_BAD_DATA'\n",
        "          print(directory)  \n",
        "          # Parent Directory path\n",
        "          parent_dir = \"/content/gdrive/MyDrive/DATASET\"\n",
        "          path = os.path.join(parent_dir, directory)\n",
        "          print(path)\n",
        "          os.mkdir(path)\n",
        "        except:\n",
        "          print(\"Error, la carpeta no se puede crear o ya existe\" )\n",
        "\n",
        "        #SE QUITA LA DATA DE LA DATASET HASTA QUE EL %DE IMÁGENES CUYO UNIQUENESS SEA <0.2 SEA MENOR A 10% \n",
        "        #Y SE ENVÍA ESTA DATA A LA CARPETA UNIQUENESS_FINE_DATA \n",
        "        if (f < 90):\n",
        "          img_index =len(dup_uniqueness_bad_data)\n",
        "          while (img_index >= 0):\n",
        "            frame_name =dup_uniqueness_bad_data[img_index-1]\n",
        "            src_path = '/content/gdrive/MyDrive/DATASET/mydataset/' + frame_name #CARPETA DE ORIGEN\n",
        "            dst_path = '/content/gdrive/MyDrive/DATASET/UNIQUENESS_BAD_DATA/' + frame_name #CARPETA DE FIN\n",
        "            try:\n",
        "              shutil.move(src_path, dst_path)\n",
        "            except:\n",
        "              print(\"Verificar si se copió: \" + dup_uniqueness_bad_data[img_index-1])\n",
        "            img_index -= 1\n",
        "       \n",
        "        #SI EL %DE IMÁGENES CUYO UNIQUENESS ES <0.2 ES MENOR A 10% SE MUEVE LA DATA\n",
        "        if (f > 90):\n",
        "          img_index =len(dup_uniqueness_fine_data)\n",
        "          while (img_index >= 0):\n",
        "            frame_name =dup_uniqueness_fine_data[img_index-1]\n",
        "            src_path = '/content/gdrive/MyDrive/DATASET/mydataset/' + frame_name #CARPETA DE ORIGEN\n",
        "            dst_path = '/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA/' + frame_name #CARPETA DE FIN\n",
        "            try:\n",
        "              shutil.move(src_path, dst_path)\n",
        "            except:\n",
        "              print(\"Verificar si se copió: \" + dup_uniqueness_fine_data[img_index-1])\n",
        "            img_index -= 1\n",
        "\n",
        "        j=1\n",
        "        #SI EL %DE IMÁGENES CUYO UNIQUENESS ES <0.2 ES MAYOR A 10% \n",
        "        while (f < 90):\n",
        "\n",
        "          fob.compute_uniqueness(self.dataset)\n",
        "\n",
        "          #Se crea el array uniqueness_fine para almacenar los valores de uniqueness\n",
        "          uniqueness_fine = [self.sample.uniqueness for self.sample in self.dataset]\n",
        "          print(uniqueness_fine)\n",
        "          p=len(uniqueness_fine) \n",
        "          print(p)\n",
        "\n",
        "          #Se crea el array dup_uniqueness_fine para almacenar los valores de uniqueness > c\n",
        "          dup_uniqueness_fine = []\n",
        "          dup_uniqueness_bad = []\n",
        "          c = 0.2  #valor de referencia para ver cuanta unicidad minima es necesaria\n",
        "\n",
        "          for v in uniqueness_fine:\n",
        "            if v >= c:\n",
        "              dup_uniqueness_fine.append(v)\n",
        "            if v < c:\n",
        "              dup_uniqueness_bad.append(v) #data uniqueness < 0.2\n",
        "\n",
        "          n=len(dup_uniqueness_fine)\n",
        "          m=len(dup_uniqueness_bad)\n",
        "          print(\"Número total de archivos al inicio:\" + str(p))\n",
        "          f=(n/p)*100\n",
        "          print(\"Number of files wich uniqueness is > 0.2: %d\" % n)\n",
        "          print(\"Percent of data that is fine > 0.2: %d\" % f )\n",
        "\n",
        "          similar_ARMA = self.dataset.sort_by(\"uniqueness\", reverse=True)\n",
        "          #Se crea el array dup_uniqueness_fine_data para \n",
        "          #pasar de los valores de uniqueness a los nombres de las fotos\n",
        "          ten_best = [x.filepath for x in similar_ARMA.limit(n)]\n",
        "          dup_uniqueness_fine_data = []\n",
        "\n",
        "          for filepath in ten_best:\n",
        "              dup_uniqueness_fine_data.append(filepath.split('/')[-1])\n",
        "          print(dup_uniqueness_fine_data)\n",
        "\n",
        "          # Sort in decreasing order of uniqueness (least unique at the end)\n",
        "          #de menor a mayor\n",
        "          similar_ARMA = self.dataset.sort_by(\"uniqueness\", reverse=False)\n",
        "\n",
        "          #Se crea el array dup_uniqueness_bad_data para \n",
        "          #pasar de los valores de uniqueness a los nombres de las fotos\n",
        "          ten_best = [x.filepath for x in similar_ARMA.limit(m)]\n",
        "          dup_uniqueness_bad_data = []\n",
        "\n",
        "          for filepath in ten_best:\n",
        "              dup_uniqueness_bad_data.append(filepath.split('/')[-1])\n",
        "          print(dup_uniqueness_bad_data)\n",
        "\n",
        "          #SE QUITA LA DATA DE LA DATASET HASTA QUE EL %DE IMÁGENES CUYO UNIQUENESS SEA <0.2 SEA MENOR A 10% \n",
        "          #Y SE ENVÍA ESTA DATA A LA CARPETA UNIQUENESS_FINE_DATA \n",
        "          if (f < 90):\n",
        "            img_index =len(dup_uniqueness_bad_data)\n",
        "            while (img_index >= 0):\n",
        "              frame_name =dup_uniqueness_bad_data[img_index-1]\n",
        "              src_path = '/content/gdrive/MyDrive/DATASET/mydataset/' + frame_name #CARPETA DE ORIGEN\n",
        "              dst_path = '/content/gdrive/MyDrive/DATASET/UNIQUENESS_BAD_DATA/' + frame_name #CARPETA DE FIN\n",
        "              try:\n",
        "                shutil.move(src_path, dst_path)\n",
        "              except:\n",
        "                print(\"Verificar si se copió: \" + dup_uniqueness_bad_data[img_index-1])\n",
        "              img_index -= 1\n",
        "        \n",
        "          #SI EL %DE IMÁGENES CUYO UNIQUENESS ES <0.2 ES MENOR A 10% SE MUEVE LA DATA\n",
        "          if (f > 90):\n",
        "            img_index =len(dup_uniqueness_fine_data)\n",
        "            while (img_index >= 0):\n",
        "              frame_name =dup_uniqueness_fine_data[img_index-1]\n",
        "              src_path = '/content/gdrive/MyDrive/DATASET/mydataset/' + frame_name #CARPETA DE ORIGEN\n",
        "              dst_path = '/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA/' + frame_name #CARPETA DE FIN\n",
        "              try:\n",
        "                shutil.move(src_path, dst_path)\n",
        "              except:\n",
        "                print(\"Verificar si se copió: \" + dup_uniqueness_fine_data[img_index-1])\n",
        "              img_index -= 1\n",
        "\n",
        "          self.dataset.match(F(\"uniqueness\")<0.2)\n",
        "          id_map = [s.id for s in self.dataset.select_fields([\"id\"])] \n",
        "\n",
        "          #ELIMINAR DUPLICADOS\n",
        "          thresh = 0.2\n",
        "          self.samples_to_remove_u = set()\n",
        "          self.dup_idxsu = np.where(similar_ARMA[idx] < thresh)[0]\n",
        "          for self.dupi in self.dup_idxsu:\n",
        "            self.samples_to_remove_u.add(id_map[self.dupi])\n",
        "          self.dataset.delete_samples(list(self.samples_to_remove_u))\n",
        "          print(\"Luego de evaluar uniqueness: %d\" % len(self.dataset))\n",
        "\n",
        "                \n",
        "        \n",
        "\n",
        "    def preprocesardata(self,img_size):\n",
        "        ####Este código es para obtener la cantidad de imagenes en el dataset y el conjunto de datos que representa cada imagen\n",
        "        #150 img_size\n",
        "        ruta_armas = \"/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA\"\n",
        "        armas_training=[]\n",
        "\n",
        "        for img in os.listdir(ruta_armas):\n",
        "          img = cv2.imread(os.path.join(ruta_armas,img))\n",
        "          img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "          img_gray_resize = cv2.resize(img_gray,(img_size,img_size))\n",
        "          armas_training.append([img_gray_resize])\n",
        "\n",
        "        #Este código es para revisar mediante visualizacion las imagenes dentro del dataset, es decir que  determinada posición coincide con lo dispuesto en mi datased\n",
        "        plt.figure()\n",
        "        plt.imshow(np.squeeze(armas_training[2]))\n",
        "        plt.colorbar()\n",
        "        plt.grid(False)\n",
        "        plt.show()  \n",
        "\n",
        "        #ESTE CODIGO ES PARA COPIAR LA DATASET HACIA OTRA CARPETA (SE REPITE POR CADA TIPO DE ARMA CON SU PROPIA CARPETA)\n",
        "        #ARMAS BLANCAS\n",
        "        ##########################\n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAB\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        #ESTE CODIGO ES PARA ELIMINAR LAS IMAGENES CA QUE NO CORRESPONDEN A LA CARPETA AB\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        import glob\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAB/CA*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAB/AC*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        #ESTE CODIGO ES PARA ELIMINAR LAS IMAGENES LA QUE NO CORRESPONDEN A LA CARPETA AB\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        pattern = r\"/content/drive/gMyDrive/DATASET/carpetaAB/LA*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        #ARMAS CORTAS\n",
        "        ##########################\n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAC\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAC/AB*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAC/LA*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        #ARMAS LARGAS\n",
        "        ##########################\n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/UNIQUENESS_FINE_DATA\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAL\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAL/CA*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAB/AC*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        # Delete file whose name starts with string 'pro'\n",
        "        pattern = r\"/content/gdrive/MyDrive/DATASET/carpetaAL/AB*\"\n",
        "        for item in glob.iglob(pattern, recursive=True):\n",
        "            os.remove(item)\n",
        "\n",
        "        #REZIZING\n",
        "        # Creación del array de armas de corto alcance\n",
        "        armas_folder_path=\"/content/gdrive/MyDrive/DATASET/carpetaAC\"\n",
        "        armas_corto_alcance=[]\n",
        "        \n",
        "        for img in os.listdir(armas_folder_path):\n",
        "            img = cv2.imread(os.path.join(armas_folder_path,img))\n",
        "            img_gray= cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img_resize= cv2.resize(img,(img_size,img_size))\n",
        "            armas_corto_alcance.append(img_resize)\n",
        "        armas_corto_alcance = np.array(armas_corto_alcance)\n",
        "        print(armas_corto_alcance.shape)\n",
        "      \n",
        "        # Creación del array de armas de largo alcance\n",
        "        armas_folder_path=\"/content/gdrive/MyDrive/DATASET/carpetaAL\"\n",
        "        armas_largo_alcance=[]\n",
        "        \n",
        "        for img in os.listdir(armas_folder_path):\n",
        "            img = cv2.imread(os.path.join(armas_folder_path,img))\n",
        "            img_gray= cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img_resize= cv2.resize(img,(img_size,img_size))\n",
        "            armas_largo_alcance.append(img_resize)\n",
        "        armas_largo_alcance = np.array(armas_largo_alcance)\n",
        "        print(armas_largo_alcance.shape)\n",
        "\n",
        "        # Creación del array de armas blancas\n",
        "        armas_folder_path=\"/content/gdrive/MyDrive/DATASET/carpetaAB\"\n",
        "        armas_blancas=[]\n",
        "        \n",
        "        for img in os.listdir(armas_folder_path):\n",
        "            img = cv2.imread(os.path.join(armas_folder_path,img))\n",
        "            img_gray= cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img_resize= cv2.resize(img,(img_size,img_size))\n",
        "            armas_blancas.append(img_resize)\n",
        "        armas_blancas = np.array(armas_blancas)\n",
        "        print(armas_blancas.shape)\n",
        "\n",
        "        images=np.concatenate([armas_corto_alcance,armas_largo_alcance,armas_blancas])\n",
        "        print(len(images))\n",
        "        self.Images=np.array(images)\n",
        "        print(self.Images.shape)\n",
        "\n",
        "        etiquetas_ALA=np.repeat(0,len(armas_largo_alcance))\n",
        "        print(len(etiquetas_ALA))\n",
        "\n",
        "        etiquetas_ACA=np.repeat(1,len(armas_corto_alcance))\n",
        "        print(len(etiquetas_ACA))\n",
        "\n",
        "        etiquetas_AB=np.repeat(2,len(armas_blancas))\n",
        "        print(len(etiquetas_AB))\n",
        "\n",
        "        class_names=['Armas largo alcance','Armas corto alcance', 'Armas blancas' ]\n",
        "        labels = np.concatenate([etiquetas_ALA, etiquetas_ACA, etiquetas_AB])\n",
        "        print(len(labels))\n",
        "        print(labels)\n",
        "        self.Labels = np.array(labels)\n",
        "        print(self.Labels.shape)\n",
        "\n",
        "    def separardata(self):\n",
        "\n",
        "        #Creación de los grupos de entrenamiento y testing para AC\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/carpetaAC/\"\n",
        "        import os\n",
        "        ldseg=np.array(os.listdir(data))\n",
        "        i=0\n",
        "        CAsize = len(ldseg)\n",
        "        for filename in ldseg:\n",
        "          o =i+1\n",
        "          dst = str(o)+\"CA\"\n",
        "          src =data + filename\n",
        "          dst =data + dst\n",
        "          os.rename(src, dst)\n",
        "          i=i+1\n",
        "        \n",
        "        #training 70%\n",
        "        #testing 10%\n",
        "        #validation 20%\n",
        "        intTest = int(CAsize*0.10)\n",
        "        intVal = 2*intTest\n",
        "        intTrain = CAsize - intVal - intTest #con esto garantizamos que las imágenes de training nunca sean menores al 70%\n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAC\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TRAINAC\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TESTAC\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/VALIDNAC\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TRAINAC/\"\n",
        "\n",
        "        for i in range(0, intTest+intVal):\n",
        "            os.remove(data+str(i+1)+\"CA\") #SE BORRA EL PRIMER 30% DE DATA PARA QUEDARSE CON EL 70% RESTANTE\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TESTAC/\"\n",
        "\n",
        "        for i in range(0, intVal):\n",
        "            os.remove(data+str(i+1)+\"CA\") #SE BORRA 20% DE LAS PRIMERA IÁGENES, \n",
        "        for i in range(intTest+intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"CA\") #SE DEJA EL OTRO 10% Y SE BORRA EL RESTO PARA QUEDARSE CON EL 10% RESTANTE   \n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/VALIDNAC/\"\n",
        "\n",
        "        for i in range(intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"CA\") #SE DEJA EL PRIMER 20% DE IMÁGENES Y SE BORRA EL RESTO DE IMÁGENES\n",
        "              \n",
        "        #Creación de los grupos de entrenamiento y testing para AB\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/carpetaAB/\"\n",
        "        import os\n",
        "        ldseg=np.array(os.listdir(data))\n",
        "        i=0\n",
        "        CAsize = len(ldseg)\n",
        "        for filename in ldseg:\n",
        "          o =i+1\n",
        "          dst = str(o)+\"AB\"\n",
        "          src =data + filename\n",
        "          dst =data + dst\n",
        "          os.rename(src, dst)\n",
        "          i=i+1\n",
        "\n",
        "        #training 70%\n",
        "        #testing 10%\n",
        "        #validation 20%\n",
        "        intTest = int(CAsize*0.10)\n",
        "        intVal = 2*intTest\n",
        "        intTrain = CAsize - intVal - intTest #con esto garantizamos que las imágenes de training nunca sean menores al 70%    \n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAB\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TRAINAB\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TESTAB\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "        \n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/VALIDNAB\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TRAINAB/\"\n",
        "\n",
        "        for i in range(0, intTest+intVal):\n",
        "            os.remove(data+str(i+1)+\"AB\") #SE BORRA EL PRIMER 30% DE DATA PARA QUEDARSE CON EL 70% RESTANTE\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TESTAB/\"\n",
        "\n",
        "        for i in range(0, intVal):\n",
        "            os.remove(data+str(i+1)+\"AB\") #SE BORRA 20% DE LAS PRIMERA IÁGENES, \n",
        "        for i in range(intTest+intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"AB\") #SE DEJA EL OTRO 10% Y SE BORRA EL RESTO PARA QUEDARSE CON EL 10% RESTANTE   \n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/VALIDNAB/\"\n",
        "\n",
        "        for i in range(intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"AB\") #SE DEJA EL PRIMER 20% DE IMÁGENES Y SE BORRA EL RESTO DE IMÁGENES\n",
        "\n",
        "        #Creación de los grupos de entrenamiento y testing para AL\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/carpetaAL/\"\n",
        "        import os\n",
        "        ldseg=np.array(os.listdir(data))\n",
        "        i=0\n",
        "        CAsize = len(ldseg)\n",
        "        for filename in ldseg:\n",
        "          o =i+1\n",
        "          dst = str(o)+\"AL\"\n",
        "          src =data + filename\n",
        "          dst =data + dst\n",
        "          os.rename(src, dst)\n",
        "          i=i+1\n",
        "\n",
        "        #training 70%\n",
        "        #testing 10%\n",
        "        #validation 20%\n",
        "        intTest = int(CAsize*0.10)\n",
        "        intVal = 2*intTest\n",
        "        intTrain = CAsize - intVal - intTest #con esto garantizamos que las imágenes de training nunca sean menores al 70%\n",
        "\n",
        "        source_dir = \"/content/gdrive/MyDrive/DATASET/carpetaAL\"\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TRAINAL\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/TESTAL\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "        \n",
        "        destination_dir = \"/content/gdrive/MyDrive/DATASET/VALIDNAL\"\n",
        "        shutil.copytree(source_dir, destination_dir)\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TRAINAL/\"\n",
        "\n",
        "        for i in range(0, intTest+intVal):\n",
        "            os.remove(data+str(i+1)+\"AL\") #SE BORRA EL PRIMER 30% DE DATA PARA QUEDARSE CON EL 70% RESTANTE\n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/TESTAL/\"\n",
        "\n",
        "        for i in range(0, intVal):\n",
        "            os.remove(data+str(i+1)+\"AL\") #SE BORRA 20% DE LAS PRIMERA IÁGENES, \n",
        "        for i in range(intTest+intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"AL\") #SE DEJA EL OTRO 10% Y SE BORRA EL RESTO PARA QUEDARSE CON EL 10% RESTANTE   \n",
        "\n",
        "        data = \"/content/gdrive/MyDrive/DATASET/VALIDNAL/\"\n",
        "\n",
        "        for i in range(intVal, CAsize):\n",
        "            os.remove(data+str(i+1)+\"AL\") #SE DEJA EL PRIMER 20% DE IMÁGENES Y SE BORRA EL RESTO DE IMÁGENES\n",
        "                    "
      ],
      "metadata": {
        "id": "BJjHx0USjXjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa63184-8199-497e-9773-8f605ee12273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Migrating database to v0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.migrations.runner:Migrating database to v0.18.0\n"
          ]
        }
      ]
    }
  ]
}